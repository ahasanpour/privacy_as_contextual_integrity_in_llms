# Privacy as Contextual Integrity in LLMs

These papers addressed Contextual Integrity in LLMs (large language models). Contributions welcome!

| Year | Paper | Dataset/Benchmark | Repository/Webpage 
|------| --- |-------------------| --- |
| 2024 | Can llms keep a secret? testing privacy implications of language models via contextual integrity theory|         CONFAIDE          |https://confaide.github.io/
| 2024 | Operationalizing Contextual Integrity in Privacy-Conscious Assistants | 
| 2024 | Large Language Models Can Be Contextual Privacy Protection Learners|| https://github.com/Yijia-Xiao/PPLM
| 2024 | GOLDCOIN: Grounding Large Language Models in Privacy Laws via Contextual Integrity Theory ||https://github.com/HKUST-KnowComp/GoldCoin


## Related Papers
| Year | Paper | Dataset/Benchmark | Repository/Webpage 
|------| --- |-------------------| --- |
|2023|FANToM: A benchmark for stress-testing machine theory of mind in interactions| FANToM            |https://hyunw.kim/fantom