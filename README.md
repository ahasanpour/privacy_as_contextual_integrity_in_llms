# Privacy as Contextual Integrity in LLMs

These papers addressed Contextual Integrity in LLMs (large language models). Contributions welcome!

| Year | Paper | Dataset/Benchmark | Repository/Webpage 
|------| --- |-------------------| --- |
| 2024 | Can llms keep a secret? testing privacy implications of language models via contextual integrity theory|         CONFAIDE          |https://confaide.github.io/
| 2024 | Operationalizing Contextual Integrity in Privacy-Conscious Assistants | 
| 2024 | Large Language Models Can Be Contextual Privacy Protection Learners|| https://github.com/Yijia-Xiao/PPLM
| 2024 | GOLDCOIN: Grounding Large Language Models in Privacy Laws via Contextual Integrity Theory |GOLDCOIN-HIPAA|https://github.com/HKUST-KnowComp/GoldCoin
|2024 | Protecting Users From Themselves: Safeguarding Contextual Privacy in Interactions with Conversational Agents| 
|2024 | LLM-CI: Assessing Contextual Integrity Norms in Language Models||

## Related Papers
| Year | Paper | Dataset/Benchmark | Repository/Webpage 
|------| --- |-------------------| --- |
|2023|FANToM: A benchmark for stress-testing machine theory of mind in interactions| FANToM            |https://hyunw.kim/fantom